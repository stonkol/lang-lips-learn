{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning\n",
        "\n",
        "## Learn + Resources\n",
        "\n",
        "1. Hands-On Machine Learning (Book)\n",
        "1. deeplearning.ai (courses)\n",
        "1. ML theory [coursera](https://www.coursera.org/specializations/machine-learning-introduction)\n",
        "1. The official Tensorflow [tutorial](https://www.tensorflow.org/tutorials). It is a bunch of Colab notebooks that give you the necessary code to get simple models running. The tutorials are one of the best resources to get clean code for Tensorflow in my opinion.\n",
        "1. This is a professional certificate on [coursera](https://www.coursera.org/professional-certificates/tensorflow-in-practice) that can help you dive directly into tensorflow with many basic tensorflow application examples.\n",
        "1. This [Kaggle](https://www.kaggle.com/learn/deep-learning) mini-course is also quite helpful to get some practice for Tensorflow. You also get a free certificate on completion!\n",
        "1. This is a [blog](https://www.pyimagesearch.com/) whose main focus is on Computer Vision(ml for images) applications of Deep Learning. The author gives out a lot of free reading material that has helped me a lot in making Computer Vision models. His content is quite engaging as well.\n",
        "\n",
        "1. Build a Virtual World Filled with Self-Driving Cars – JavaScript [yt](https://www.youtube.com/watch?app=desktop&v=5iHejdqYIa8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choosing ML\n",
        "[Now](https://www.reddit.com/r/learnmachinelearning/comments/18lpdt2/tensorflow_or_pytorch/) in general `Tensorflow` is no longer preferred. It's either `PyTorch` or `Jax`. `Tensorflow` is a little gentler on the user for an introduction whereas `PyTorch` can screw with new users. \n",
        "\n",
        "Most of the time `Tensorflow` is all you really need. It is unlikely that you will be doing anything outside the standard layer types.\n",
        "\n",
        "The table is based on: [link](https://www.v7labs.com/blog/pytorch-vs-tensorflow)\n",
        "|                       | PyTorch               | TensorFlow (Keras)|\n",
        "| --------------------- | --------------------- | ------------  |\n",
        "| **by**                | Meta                  | Google        |\n",
        "| **Learning curve**    | Higher                | **Lower**     |\n",
        "| **Usage**             | **70%**               | <5%           |\n",
        "| **Available Models**  | **More**              | Less          |\n",
        "| **API Level**         | High                  | Low & High    |\n",
        "| **Oriented**          | Research              | Production    |\n",
        "| **Pros**              | **Wide adoption** by the AI research community. Trending and Gaining Traction. Used by Top Academic institutions. | More complete production ecosystem with TensorFlow Serving, TFLite, TFX, and multiple language support. Keras allows rapid experimentation. You can build MLOps pipelines with TFX. |\n",
        "| **Cons**               | Not as complete in terms of production-ready tools for end-to-end projects.| Small research community. Less compatible Transformer models on HuggingFace.      |\n",
        "| **Programming Language Availability**   | Python     | Python, Javascript, C++, Java, _(Go, Swift)_  |\n",
        "| **Key Characteristics**| Very Pythonic and flexible.     | Fast model creation and deployment.|\n",
        "| **Difficulty**         | Steeper learning curve. Very intuitive once you learn it. | Very easy if Keras is used for modeling. Otherwise, it has a steep learning curve.   |\n",
        "| **Popularity**         | It’s gaining huge popularity both among researchers and practitioners in the industry. | It still remains the most popular deep learning framework. _(171k vs. 62.6k in GitHub stars)_ |\n",
        "| **Trending**           | Big influx of new users over the last years.  | Not trending anymore.|\n",
        "| **Ecosystem**          | Rich ecosystem, mostly maintained by the community, oriented towards research and modeling. | Rich native ecosystem, mostly oriented towards production MLOps. |\n",
        "\n",
        "> [!NOTE] Adaptation is key to surviving in what is probably the fastest-moving industry at the moment. Don’t get too fixated on a single framework or tool; it's important to be proficient in multiple technologies and understand their pros and cons. *Not all problems are nails, so not every tool should be a hammer!*\n",
        "\n",
        "\n",
        "[Exploring Deep Learning Frameworks: PyTorch vs. TensorFlow](https://medium.com/@ghostsmaw/exploring-deep-learning-frameworks-pytorch-vs-tensorflow-1089d2cb9580)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ML setup on macOS (TensorFlow)\n",
        "\n",
        "Setup Mac for Machine Learning with TensorFlow [yt video](https://www.youtube.com/watch?app=desktop&v=_1CaUOHhI6U). And here is the [Instructions and code](https://www.youtube.com/watch?app=desktop&v=_1CaUOHhI6U) of the video. Some of the steps that it has:\n",
        "1. It set up a conda virtual environment for running ML\n",
        "1. Install base TensorFlow ([Apple's fork](https://developer.apple.com/metal/tensorflow-plugin/) of TensorFlow is called `tensorflow-macos`).\n",
        "1. Install apple silicon dependencies for Tensorflow.\n",
        "1. Install Tensorflow Metal\n",
        "1. (Optional) Install TensorFlow Datasets to run benchmarks\n",
        "1. Install common data science packages. \n",
        "1. Start Jupyter Notebook. `$ jupyter notebook`\n",
        "\n",
        "> I have errors after I start Jupyter Notebook, I enter `$ pip3 install chardet` for solving the problem.\n",
        "8. Then I can create a new NoteBook\n",
        "9. Import dependencies and check TensorFlow version/GPU access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "10. It will return: \n",
        "\n",
        "    TensorFlow has access to the following devices:\n",
        "    [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
        "    TensorFlow version: 2.15.0\n",
        "    - Tensorflow Metal [repo](https://pypi.org/project/tensorflow-metal/), to check if you have the latest version\n",
        "11. After finished, go to File > Shut Down (you should do this in the note and also in the jupyter file manager page)\n",
        "12. Exit \"Base\" environment:\" `$ conda config --set auto_activate_base false`. this will make conda's base environment not be activated on startup. \n",
        "13. Restart the terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check for TensorFlow GPU access\n",
        "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
        "\n",
        "# See TensorFlow version\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### What is?\n",
        "\n",
        "Note from the yt course [TensorFlow 2.0 Complete Course - Python Neural Networks](https://www.youtube.com/watch?v=tPYj3fFJGjk)\n",
        "\n",
        "#### AI vs ML vs NN\n",
        "With **AI** you need to write `the rules`\n",
        "\n",
        "With **ML** it can figure it out `the rules` by the inputs you are giving and output the data that should be. It require more data to train a Model.\n",
        "\n",
        "Neural Network (**NN**)(Deep Learning) have more than two layers (input and output), there are multiplier layers and the input data will be transformed throughout the layers until it get an output. A `Multi Stage Information Extraction`. NN is **NOT** modelled after the brain, because we do not how the brains really works, only maybe a little bit inspired.\n",
        "\n",
        "#### Classical vs ML Programming\n",
        "Classical Programming: input `Data`+`Rules` -output-> `Answers`\n",
        "ML Programming: input `Data`+`Answers` -output-> `Rules`\n",
        "\n",
        "ML are not 100% accuracy, but the goal is to have the highest accuracy possible.\n",
        "\n",
        "### Types of Learning\n",
        "#### Supervised\n",
        "Human will Help it to improve the accuracy \n",
        "\n",
        "#### UnSupervised\n",
        "Advantage when you have a ton of information\n",
        "\n",
        "#### Reinforcement Learning\n",
        "The `agent` will explore the `environment`, if it is doing correct you will `reward` him, then the `agent` will be figure it out how it works.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Terminology\n",
        "\n",
        "Feature = Input data\n",
        "Label = Output information\n",
        "\n",
        "Scalar value: one value\n",
        "Vector value: muliple values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TensorFlow\n",
        "### What is?\n",
        "Maintained by Google. Is a easy to learn, don't need to much math knowledge, only some basic calculus and linear algebra at the beginning, and then can have gradient descent, regression techniques and classifications, but most of the cases you only need the basics. \n",
        "\n",
        "\n",
        "Import init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3.12' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "\n",
        "# %tensorflow_version 2.x #use it if you are in a notebook\n",
        "\n",
        "import tensorflow as tf \n",
        "print(tf.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- Has 2 main components: graphs and sessions\n",
        "\n",
        "#### Graphs\n",
        "\n",
        "When we write code in TF we create a graph.\n",
        "\n",
        "we write a variable, then the var will be added to the graph, the graph will define for example that var1 = var2 + var3, but it will not run or evaluate your var or code, we only now that var1 = 3 + 4, but we dont know that var1 is 7. So in graph we only store partial computation. \n",
        "\n",
        "#### Sessions\n",
        "\n",
        "Is a way to execute part of your entire graph. When it runs it will start executing different aspects of your graph. It will start from the lowest part of your graph. It allocates memory and resources and handles the execution of the operations and computations we have defined.\n",
        "\n",
        "#### Tensor\n",
        "\n",
        "when we create out program, will be going to creating a bunch of tensors and they will gping to store partially defined computations in the graph\n",
        "\n",
        "Each Tensor has a data type and shape\n",
        "**Data Types include**: `float32`, `int32`,  `string` and others.\n",
        "**Shape**: represents the dimension of data.\n",
        "\n",
        "\n",
        "#### Creating Tensors "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "string = tf.Variable(\"this is a string, tf.string\")\n",
        "number = tf.Variable(324, tf.int16)\n",
        "floating = tf.Variable(3.567, tf.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rank/Degree of Tensors\n",
        "the number of dimensions involve in a tensor. \n",
        "One value var like the `string` above is 0 (is like 0 dimension, it even have 1 dimension(1D))\n",
        "When is an array we have rank 1 (analogous like 1 dimension, can go left or right)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rank1_tensor = tf.Variable([\"Test\"], tf.string)  #an array, even is only one var inside, you can add more: [\"Test\", \"kiwi\", \"cake\"]\n",
        "rank2_tensor = tf.Variable([[\"test\", \"ok\"], [\"go\", \"yeah\"]], tf.string) #a matrix, double array, 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.rank1_tensor(rank2_tensor) #show rank level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shape of a Tensor\n",
        "amounts of elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rank2_tensor.shape # [2, 2]  #show shape of a tensor\n",
        "\n",
        "tensor1 = tf.ones([1,2,3])  #creates a shape [1,2,3] tensor full of ones\n",
        "tensor2 = tf.reshape(tensor1, [2,3,1]) #reshape existing data to shape[2,3,1]\n",
        "tensor3 = tf.reshape(tensor1, [3, -1]) # -1 tells the tensor to calculate the size of the dimension in that place, this will reshape the tensor to [3,2]\n",
        "\n",
        "# tensor1 = [[1,1,1]] [[1,1,1]]\n",
        "# tensor2 = [[1],[1],[1]] [[1],[1],[1]]\n",
        "# tensor3 = [1,1] [1,1] [1,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "there are var and const tensors.\n",
        "\n",
        "### Evaluating Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tf.Session() as sess:  # creates a session using default graph\n",
        "    tensor.eval()           # tensor will of course be the name of your tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = tf.zeros([5,5,5,5])\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "it will print five times of five blocks of these (5x5x5 lines):\n",
        "```python\n",
        "[[0,0,0,0,0]\n",
        "[0,0,0,0,0]\n",
        "[0,0,0,0,0]\n",
        "[0,0,0,0,0]\n",
        "[0,0,0,0,0]\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = tf.reshape(t, [625])\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this will re arrange all the zeros in all those blocks and meta arrays to a super big one array, (print a continous of 625 zeros):\n",
        "```python\n",
        "[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,,0,0.......]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = tf.reshape(t, [625], -1)\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this will re arrange it to 125 arrays, which each of those subarrays have 5 zeroes:\n",
        "\n",
        "```python\n",
        "[[0,0,0,0,0]\n",
        "[0,0,0,0,0]\n",
        "...\n",
        "[0,0,0,0,0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3: Algorithm\n",
        "\n",
        "including:\n",
        "1. Linear Regression\n",
        "1. Classification\n",
        "1. Clustering\n",
        "1. Hidden Markov Models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Linear Regression\n",
        "One of the most basic forms of ML and is used to predict numeric values.\n",
        "Come out a module to solve the relation of a data set(like in a x,y graph). And can predict future values. It should correlate linearly\n",
        "\n",
        "`y = mx + b`\n",
        "\n",
        "You will give some data (x,y), and the ml will solve the m and b.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np  #a very optimize version of array, very quickly perform data, manipulate, vector operations....\n",
        "import pandas as pd     # data manipulation, visualize, cut out rows, columns...\n",
        "import matplotlib.pyplot as plt   #graphs dataset\n",
        "six.moves import urllib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dftrain = pd.read_csv('a_file_for_training.csv')\n",
        "dfeval = pd.read_csv('a_file_for_evaluation.csv')\n",
        "y_train = dftrain.pop('survived')\n",
        "y_train = dfeval.pop('survived')\n",
        "\n",
        "CATEGORICAL_COLUMNS = ['sex', 'n_siblings', 'parch', 'class', 'deck', 'embark_town']\n",
        "\n",
        "NUMERIC_COLUMNS = ['age', 'fare']\n",
        "\n",
        "for feature_name in CATEGORICAL_COLUMNS:\n",
        "    vocabulary = dftrain[feature_name].unique() #gets a list of all unique values from given features column\n",
        "    feature_column.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n",
        "\n",
        "for feature_name in NUMERIC_COLUMNS:\n",
        "    feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n",
        "\n",
        "print(feature_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The training process\n",
        "Usually the memory is not that big so we train it in batches.\n",
        "\n",
        "An epoch is simply one stream of our entire dataset. The number of epochs we define is the amount of times our model will see the entire dataset.\n",
        "\n",
        "The number of epochs we define is the amount of times our model will see the entire dataset. We use multiple epochs in hope that after seeing the same data multiple times the model will better determine how to estimate it.\n",
        "\n",
        "### Input Function\n",
        " Is the way we define our data will be broken into epochs and into batches to feed our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_function():  # inner function, this will be returned\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000)  # randomize order of data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
        "    return ds  # return a batch of the dataset\n",
        "  return input_function  # return a function object for use\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a mL engineer you should aim a high accuracy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = list(linear_est.predict(eval_input_fn))\n",
        "print(result[0]['probabilities'])  # print probability for survive\n",
        "\n",
        "# then\n",
        "# you can compare print someone info\n",
        "# and what he predict if he/she has survived"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### classification\n",
        "You now predict classes, instead of a data point we are going to call it within all of the the different classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering \n",
        "involves in the grouping of data points. In theory same groups should have similar properties or features.\n",
        "It finds clusters, it tells you the location of those clusters\n",
        "You can tell how many clusters you wnat to find. the algortih find those datasets for you\n",
        "\n",
        "basic algorithm for k-means:\n",
        "1. Randomly pick K points to place K centroids(where the current clusters are defined )\n",
        "2. Assign all the data points by distance. The closest centroid to a point is the one it is assigned to.\n",
        "3. Average al the data points belonging to each centroid to find the middle of those clusters (center of mass). Place the corresponding centroids into that position.\n",
        "4. Reassign every point once again to the closest centroid.\n",
        "5. Repeat steps 3~4 until no point changes which centroid it belongs to.\n",
        "center of mass = center of a group of data points\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
